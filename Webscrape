import pandas as pd
import requests
from bs4 import BeautifulSoup as soup

header = {
    'Origin': 'https://www.1mg.com',
    'Referer': 'https://www.1mg.com/drugs-all-medicines?page=1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'
}

def scrape_data(url):
    html = requests.get(url=url, headers=header)
    html.status_code

    bsobj = soup(html.content, 'lxml')

    product_name = []
    for name in bsobj.findAll('div', {'itemprop': 'name'}):
        product_name.append(name.text.strip())

    pack_size = []
    for size in bsobj.findAll('div', {'class': 'style__padding-bottom-5px___2NrDR'}):
        pack_size.append(size.text.strip())

    data = pack_size
    titles = []
    costs = []
    dosages = []
    manufacturers = []
    compositions = []

    for i in range(0, len(data), 4):
        title_cost = data[i].rsplit('MRPâ‚¹', 1)
        titles.append(title_cost[0].strip())
        costs.append(title_cost[1].strip() if len(title_cost) > 1 else '')

        dosages.append(data[i + 1])
        manufacturers.append(data[i + 2])
        compositions.append(data[i + 3])

    df = pd.DataFrame({
        'Title': titles,
        'Cost': costs,
        'Dosage': dosages,
        'Manufacturer': manufacturers,
        'Composition': compositions
    })

    return df

def scrape_and_save(base_url, page_range, label_range, output_filename):
    result = []

    for page_num in page_range:
        for label in label_range:
            current_url = f"{base_url}?page={page_num}&label={chr(label)}"
            df = scrape_data(current_url)
            result.append(df)

    # Concatenate all DataFrames
    final_df = pd.concat(result, ignore_index=True)

    # Save to Excel
    final_df.to_excel(output_filename, index=False)
    print(f"Data saved to {output_filename}")

if __name__ == "__main__":
    base_url = "https://www.1mg.com/drugs-all-medicines"
    page_range = range(81,90)
    label_range = range(ord('a'), ord('z')+1)
    output_filename = "output_combined-10.xlsx"

    scrape_and_save(base_url, page_range, label_range, output_filename)
