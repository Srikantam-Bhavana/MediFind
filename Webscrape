pip install autoscraper

import html
from urllib.request import Request,urlopen
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import json

base_url="https://www.1mg.com/drugs-all-medicines?label="
li=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']
data=[]

import requests

response = requests.get('http://httpbin.org/headers')

print(response.status_code)
print(response.text)

for i in li:
    print("Page " + i + " scraping started!!!")

    url = base_url + i
    req = Request(url, headers={'User-Agent': 'python-requests/2.24.0'})
    webpage = urlopen(req).read()
    page_soup = BeautifulSoup(webpage, "html.parser")
    ingredients = page_soup_recipe.find('div', class_='style_font-bold_1k9Dl stylefont-14px_YZZrf styleflex-row_2AKyf stylespace-between_2mbvn stylepadding-bottom-5px__2NrDR').find_all('div')
    temp_ingredients = [ingredient.text.strip() for ingredient in ingredients]
    recipe_ingredients.append(temp_ingredients)
    print(temp_ingredients)
    

from urllib.request import Request, urlopen
from bs4 import BeautifulSoup

base_url = "https://www.1mg.com/drugs-all-medicines?page="
li = ["1", "2", "3"]  # replace with your actual page values

for i in li:
    print("Page " + i + " scraping started!!!")

    url = base_url + i
    req = Request(url, headers={'User-Agent': 'python-requests/2.24.0'})
    webpage = urlopen(req).read()
    page_soup = BeautifulSoup(webpage, "html.parser")

    medicine_names = [item.text.strip() for item in page_soup.find_all('div', class_='style_font-bold_1k9Dl stylefont-14px_YZZrf styleflex-row_2AKyf stylespace-between_2mbvn stylepadding-bottom-5px__2NrDR')]
    prices = [item.text.strip() for item in page_soup.find_all('div', class_='style_font-normal_2gZqF stylemargin-left-8px__3Sw1d')]
    quantities = [item.text.strip() for item in page_soup.find_all('div', class_='style_padding-bottom-5px__2NrDR')]
    manufacturers = [item.text.strip() for item in page_soup.find_all('div', class_='style_padding-bottom-5px__2NrDR')]
    compositions = [item.text.strip() for item in page_soup.find_all('div', class_='style_font-12px_2ru_e styleproduct-content_5PFBW styledisplay-inline-block__2y7gd')]

    # Now you have lists containing the extracted data for each class
    print("Medicine Names:", medicine_names)
    print("Prices:", prices)
    print("Quantities:", quantities)
    print("Manufacturers:", manufacturers)
    print("Compositions:", compositions)


pip install selenium





base_url = 'https://www.epicurious.com/search?content=recipe&page='
prefix_url = 'https://www.epicurious.com'

recipe_titles = []
recipe_img_links = []
recipe_ingredients = []
recipe_instructions = []

IMGS_FOLDER = "images/epi/"

count=1
for i in range(1, 2):  # Adjust the range as needed
    print("Page " + str(i) + " scraping started!!!")
    
    url = base_url + str(i)
    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    webpage = urlopen(req).read()
    page_soup = BeautifulSoup(webpage, "html.parser")
    
    recipe_links_on_page = []
    
    recipe_links = page_soup.find_all('h4', {'class': 'hed'})
    print(recipe_links)
    

    for link in recipe_links:
        recipe_links_on_page.append(prefix_url + link.find('a')['href'])
        
        # Convert the Beautiful Soup object to string
        link_html = str(link)  
        
        # Use BeautifulSoup on the HTML string
        soup = BeautifulSoup(link_html, 'html.parser')  

        # Finding the <a> tag within <h4> tag
        a_tag = soup.find('a')

        # Extracting text from the <a> tag
        if a_tag:
            recipe_name = a_tag.text
            recipe_titles.append(recipe_name)
        else:
            # If <a> tag not found, add "N/A"
            recipe_titles.append("N/A")  

# print(recipe_links_on_page)
    for recipe_url in recipe_links_on_page:
        req_recipe = Request(recipe_url, headers={'User-Agent': 'Mozilla/5.0'})
        webpage_recipe = urlopen(req_recipe).read()
        page_soup_recipe = BeautifulSoup(webpage_recipe, "html.parser")
        
        # Ingredients
        ingredients = page_soup_recipe.find('div', class_='List-iSNGTT ljAYJm').find_all('div')
        temp_ingredients = [ingredient.text.strip() for ingredient in ingredients]
        recipe_ingredients.append(temp_ingredients)
        print(temp_ingredients)


        # Recipe Instructions
        instructions = page_soup_recipe.find('div', class_='InstructionsWrapper-hZXqPx RmryN').find('ol').find('li').find_all('p')
        temp_instructions = [instruction.text.strip() for instruction in instructions]
        recipe_instructions.append(temp_instructions)
#         print(recipe_intructions)

        # Creating JSON object for individual recipe
        entry = {
            'Recipe_number': count,
            'Recipe_name': recipe_titles[count-1],
            'Recipe_ingredients': temp_ingredients,
            'Recipe_instructions': temp_instructions,
#             'Recipe_img_link': temp_img
        }

        # Append to JSON file
        with open("dataset_epi.json", mode='a', encoding='utf-8') as feedsjson:
            json.dump(entry, feedsjson)
            feedsjson.write('\n')
        count+=1
        
#         print("Recipe " + str(len(recipe_titles)) + " Scrapped")
    print("Page "+str(i)+" Scraped!!!")

#!/usr/bin/env python
# coding: utf-8

# # Web Scraping 1mg

# In[1]:

import pandas as pd
import requests
from bs4 import BeautifulSoup as soup


# In[2]:


header = {'Origin': 'https://www.1mg.com',
'Referer': 'https://www.1mg.com/drugs-all-medicines?page=1',
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'
}


# In[3]:


url = 'https://www.1mg.com/drugs-all-medicines?page=1'


# In[4]:


html = requests.get(url=url,headers=header)
html.status_code


# In[5]:


bsobj = soup(html.content,'lxml')
bsobj


# In[6]:


bsobj.findAll('div',{'itemprop':'name'})



product_name = []
for name in bsobj.findAll('div',{'itemprop':'name'}):
    product_name.append(name.text.strip())
    
print(product_name)


pack_size = []
#style__padding-bottom-5px___2NrDR
#composition : style__padding-bottom-5px___2NrDR style__display-table___226Zq
#
for size in bsobj.findAll('div',{'class':'style__padding-bottom-5px___2NrDR'}):
    pack_size.append(size.text.strip())
    


# In[11]:


print(pack_size)


# Splitting each item into three parts
rows = [pack_size[i:i+3] for i in range(0, len(pack_size), 3)]

# Splitting MRP from the value
for i in range(len(rows)):
    rows[i][1] = rows[i][1].replace('MRP₹', '')

# Creating a DataFrame
df = pd.DataFrame(rows, columns=['Name', 'Description', 'Manufacturer'])

# Saving the DataFrame to Excel
df.to_excel('akpak1.xlsx', index=False)


import requests
from bs4 import BeautifulSoup as soup

header = {
    'Origin': 'https://www.1mg.com',
    'Referer': 'https://www.1mg.com/drugs-all-medicines?page=1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'
}

url = 'https://www.1mg.com/drugs-all-medicines?page=1'
html = requests.get(url=url, headers=header)

if html.status_code == 200:
    bsobj = soup(html.content, 'html.parser')
    pack_size = []

    for size in bsobj.select('.style_padding-bottom-5px__2NrDR'):
        

    print(pack_size)
else:
    print(f"Failed to retrieve the page. Status code: {html.status_code}")


import pandas as pd
import requests
from bs4 import BeautifulSoup as soup

header = {
    'Origin': 'https://www.1mg.com',
    'Referer': 'https://www.1mg.com/drugs-all-medicines?page=1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'
}

def scrape_data(url):
    html = requests.get(url=url, headers=header)
    html.status_code

    bsobj = soup(html.content, 'lxml')

    product_name = []
    for name in bsobj.findAll('div', {'itemprop': 'name'}):
        product_name.append(name.text.strip())

    pack_size = []
    for size in bsobj.findAll('div', {'class': 'style__padding-bottom-5px___2NrDR'}):
        pack_size.append(size.text.strip())

    data = pack_size
    titles = []
    costs = []
    dosages = []
    manufacturers = []
    compositions = []

    for i in range(0, len(data), 4):
        title_cost = data[i].rsplit('MRP₹', 1)
        titles.append(title_cost[0].strip())
        costs.append(title_cost[1].strip() if len(title_cost) > 1 else '')

        dosages.append(data[i + 1])
        manufacturers.append(data[i + 2])
        compositions.append(data[i + 3])

    df = pd.DataFrame({
        'Title': titles,
        'Cost': costs,
        'Dosage': dosages,
        'Manufacturer': manufacturers,
        'Composition': compositions
    })

    return df

def scrape_and_save(base_url, page_range, label_range, output_filename):
    result = []

    for page_num in page_range:
        for label in label_range:
            current_url = f"{base_url}?page={page_num}&label={chr(label)}"
            df = scrape_data(current_url)
            result.append(df)

    # Concatenate all DataFrames
    final_df = pd.concat(result, ignore_index=True)

    # Save to Excel
    final_df.to_excel(output_filename, index=False)
    print(f"Data saved to {output_filename}")

if __name__ == "__main__":
    base_url = "https://www.1mg.com/drugs-all-medicines"
    page_range = range(1, 100)
    label_range = range(ord('a'), ord('z')+1)
    output_filename = "output_combined.xlsx"

    scrape_and_save(base_url, page_range, label_range, output_filename)


df_cleaned = df[~df.apply(lambda row: row.astype(str).str.contains('\+').any(), axis=1)]


import pandas as pd
import requests
from bs4 import BeautifulSoup as soup

header = {
    'Origin': 'https://www.1mg.com',
    'Referer': 'https://www.1mg.com/drugs-all-medicines?page=1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'
}

def scrape_data(url):
    html = requests.get(url=url, headers=header)
    html.status_code

    bsobj = soup(html.content, 'lxml')

    product_name = []
    for name in bsobj.findAll('div', {'itemprop': 'name'}):
        product_name.append(name.text.strip())

    pack_size = []
    for size in bsobj.findAll('div', {'class': 'style__padding-bottom-5px___2NrDR'}):
        pack_size.append(size.text.strip())

    data = pack_size
    titles = []
    costs = []
    dosages = []
    manufacturers = []
    compositions = []

    for i in range(0, len(data), 4):
        title_cost = data[i].rsplit('MRP₹', 1)
        titles.append(title_cost[0].strip())
        costs.append(title_cost[1].strip() if len(title_cost) > 1 else '')

        dosages.append(data[i + 1])
        manufacturers.append(data[i + 2])
        compositions.append(data[i + 3])

    df = pd.DataFrame({
        'Title': titles,
        'Cost': costs,
        'Dosage': dosages,
        'Manufacturer': manufacturers,
        'Composition': compositions
    })

    return df

def scrape_and_save(base_url, page_range, label_range, output_filename):
    result = []

    for page_num in page_range:
        for label in label_range:
            current_url = f"{base_url}?page={page_num}&label={chr(label)}"
            df = scrape_data(current_url)
            result.append(df)

    # Concatenate all DataFrames
    final_df = pd.concat(result, ignore_index=True)

    # Save to Excel
    final_df.to_excel(output_filename, index=False)
    print(f"Data saved to {output_filename}")

if __name__ == "__main__":
    base_url = "https://www.1mg.com/drugs-all-medicines"
    page_range = range(1, 6)
    label_range = range(ord('a'), ord('z')+1)
    output_filename = "output_combined1.xlsx"

    scrape_and_save(base_url, page_range, label_range, output_filename)


import requests
import hashlib

def calculate_checksum(data):
    # Calculate the checksum (hash) of the data
    sha256 = hashlib.sha256()
    sha256.update(data.encode('utf-8'))
    return sha256.hexdigest()

def compare_checksums(original_data, scraped_data):
    # Calculate checksums for both original and scraped data
    original_checksum = calculate_checksum(original_data)
    scraped_checksum = calculate_checksum(scraped_data)

    # Compare checksums
    if original_checksum == scraped_checksum:
        print("Checksums match. Data is authentic.")
    else:
        print("Checksums differ. Potential data discrepancies.")

# Example usage
original_data = requests.get("https://www.1mg.com/drugs-all-medicines").text
scraped_data = scrape_data("output_combined.xlsx")

compare_checksums(original_data, scraped_data)


import pandas as pd

# Load the two Excel sheets into DataFrames
df1 = pd.read_excel('path/to/sheet1.xlsx')
df2 = pd.read_excel('path/to/sheet2.xlsx')

# Compare the two DataFrames
comparison_result = df1.equals(df2)

# Display the result
if comparison_result:
    print("The two Excel sheets are identical.")
else:
    print("The two Excel sheets are different.")


import pandas as pd

# Assuming df is your DataFrame with the scraped data

# Check for missing values in the entire DataFrame
missing_values = df.isnull().sum()
print("Missing Values:")
print(missing_values)

# Validate numeric columns
numeric_columns = df.select_dtypes(include='number')
print("Numeric Columns:")
print(numeric_columns)

# Validate categorical columns
categorical_columns = df.select_dtypes(exclude='number')
print("Categorical Columns:")
print(categorical_columns)

# Check for duplicate rows
duplicate_rows = df[df.duplicated()]
print("Duplicate Rows:")
print(duplicate_rows)

# Check for unique values in each column
unique_values_per_column = df.nunique()
print("Unique Values per Column:")
print(unique_values_per_column)

# Validate specific conditions (example: 'Cost' should be numeric)
valid_cost = pd.to_numeric(df['Cost'], errors='coerce').notnull()
invalid_cost_entries = df[~valid_cost]
print("Invalid 'Cost' Entries:")
print(invalid_cost_entries)

# More advanced validations based on your specific data and requirements

# Save the cleaned DataFrame to a new Excel file
cleaned_filename = "cleaned_data.xlsx"
df_cleaned.to_excel(cleaned_filename, index=False)
print(f"Cleaned Data saved to {cleaned_filename}")
